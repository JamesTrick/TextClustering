{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Analysis Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As guidance for this work, I refer largely to an Email Clustering Thesis https://www.scribd.com/document/21970433/Email-clustering-algorithm, and a pipline referred to in the Wikipedia article on Text Clustering.\n",
    "\n",
    "This notebook starts off by taking example text data and does the following:\n",
    "* Cleans data (Removes stopwords, stems, etc.)\n",
    "* Tokenizes words, might TF-IDF it\n",
    "* Performs Dimension Reduction (PCA algorithm)\n",
    "* Performs Cluster Analysis\n",
    "\n",
    "In this notebook, a number of clustering algorithms are presented such as K-Means, MeanShift, and finally we use a Non-negative Matrix Factorisation technique for topic identification. The purpose of comparing algorithms isn't necessarily to rule them out, instead it's more about trying to get a working example of each one.\n",
    "\n",
    "For the example dataset used dimension reduction isn't really needed as the size of the dataset is unlikely to be computationally intensive, however it sets the standard for any future use of larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "import nltk\n",
    "\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this example, we'll just use Sklearn's 20 newsgroups dataset http://scikit-learn.org/stable/datasets/twenty_newsgroups.html#newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = [\n",
    "    'talk.politics.mideast',\n",
    "    'rec.motorcycles',\n",
    "]\n",
    "\n",
    "remove = ('headers', 'footers', 'quotes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                               random_state=42,\n",
    "                               remove=remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               random_state=42,\n",
    "                               remove=remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'\\n\\nYep...I think it\\'s the only CB750 with a 630 chain.\\nAfter 14 years, it\\'s finally stretching into the \"replace\" zone.\\n\\n\\n<Sigh> I know .... I know.',\n",
       " u\": \\n: While you brought up the separate question of Israel's unjustified\\n: policies and practices, I am still unclear about your reaction to\\n: the practices and polocies reflected in the article above.\\n: \\n: Tim\\n\\nNot a separate question Mr. Clock. It is deceiving to judge the \\nresistance movement out of the context of the occupation.\",\n",
       " u'\\n\\n\\tI\\'m not sure that\\'s true.  Let me rephrase; \"You can file a complaint\\n which will bring the person into court.\"  As I understand it, a\\n \"citizens arrest\" does not have to be the physical detention of\\n the person.\\n\\n Better now?',\n",
       " u'\\nI bought my Moto Guzzi from a Univ of Va grad student in Charlottesville\\nlast spring.\\n\\n\\t     Mark Cervi, cervi@oasys.dt.navy.mil, (w) 410-267-2147\\n\\t\\t DoD #0603  MGNOC #12998  \\'87 Moto Guzzi SP-II\\n      \"What kinda bikes that?\" A Moto Guzzi. \"What\\'s that?\" Its Italian.\\n-- ']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = data_train.data\n",
    "train[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "\n",
    "As we can see from the above, the data given is quite messy and has a formatting, punctuation, stopwords, etc that we'd like to parse out before running our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nYep...I think it's the only CB750 with a 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>: \\n: While you brought up the separate questi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\tI'm not sure that's true.  Let me rephra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nI bought my Moto Guzzi from a Univ of Va gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I was just wondering if there were any law off...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 col\n",
       "0  \\n\\nYep...I think it's the only CB750 with a 6...\n",
       "1  : \\n: While you brought up the separate questi...\n",
       "2  \\n\\n\\tI'm not sure that's true.  Let me rephra...\n",
       "3  \\nI bought my Moto Guzzi from a Univ of Va gra...\n",
       "4  I was just wondering if there were any law off..."
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'col':train})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['col'] = df['col'].str.lower().str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove stopwords from text using NLTK.\n",
    "stop = stopwords.words('english')\n",
    "df['col'] = df['col'].apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem words for better model performance\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "df['col'] = df['col'].apply(lambda x: [stemmer.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[yep...i, think, it, cb750, 630, chain., 14, y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[:, :, brought, separ, question, israel, unjus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[i'm, sure, that, true., let, rephrase;, \"you,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[bought, moto, guzzi, univ, va, grad, student,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[wonder, law, offic, read, this., sever, quest...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 col\n",
       "0  [yep...i, think, it, cb750, 630, chain., 14, y...\n",
       "1  [:, :, brought, separ, question, israel, unjus...\n",
       "2  [i'm, sure, that, true., let, rephrase;, \"you,...\n",
       "3  [bought, moto, guzzi, univ, va, grad, student,...\n",
       "4  [wonder, law, offic, read, this., sever, quest..."
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.to_csv('clean_news_data.csv')\n",
    "\n",
    "## Export to CSV for further cleaning in Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('clean_news_data.csv', usecols=[1],\n",
    "                           names=['text'],\n",
    "                           header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df['text'] = df['text'].str.replace(r'\\W', ' ', case = False)\n",
    "df['text'] = df['text'].str.replace(r'[.,?<>-]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brought separ question israel unjustifi   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uim sure that true let rephrase  yo file compl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bought moto guzzi univ va grad student charlot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wonder law offic read this sever question woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>theori holler kill spirit criminal nazi armeni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0      brought separ question israel unjustifi   ...\n",
       "1  uim sure that true let rephrase  yo file compl...\n",
       "2  bought moto guzzi univ va grad student charlot...\n",
       "3  wonder law offic read this sever question woul...\n",
       "4  theori holler kill spirit criminal nazi armeni..."
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The head of the dataframe now shows that the text has been cleaned pretty well! There are no clear stopwords, symbols and words have stemmed, eg Officer -> Offic etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=2, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_k = 2\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:  bike  it  one  get  like  ride  would  know  go  udont\n",
      "Cluster 1:  israel  armenian  isra  arab  jew  peopl  turkish  would  said  kill\n",
      "\n",
      "\n",
      "Prediction\n",
      "[0]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Prediction\")\n",
    "\n",
    "Y = vectorizer.transform([\"This Motorbike has the best chain\"])\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)\n",
    "\n",
    "Y = vectorizer.transform([\"Turkey is close to Israel\"])\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_means_labels = model.labels_\n",
    "k_means_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MeanShift Clustering\n",
    "\n",
    "MeanShift clustering is another centriod based clustering alogrithim that works in a similar way to the above K-Means. The advantage of Meanshift is that you don't have to specify the number of clusters prior to analysis. This is has a disctinct advantage over K-Means as when analysing general KAs, we're not sure how many natural clusters there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift, estimate_bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bandwidth = estimate_bandwidth(X.toarray(), quantile=0.2) ## Do PCA or other dimension reduction for real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_shift_model = MeanShift(bandwidth=bandwidth, bin_seeding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MeanShift(bandwidth=1.3939162198773161, bin_seeding=True, cluster_all=True,\n",
       "     min_bin_freq=1, n_jobs=1, seeds=None)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_shift_model.fit(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = mean_shift_model.labels_\n",
    "cluster_centers = mean_shift_model.cluster_centers_\n",
    "\n",
    "labels_unique = np.unique(labels)\n",
    "n_clusters = len(labels_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print n_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MeanShift at the moment, doesn't look fully helpful if it's only yielding a single cluster... Might need work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative Matrix Factorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As NMF is typically a dimension reduction technique in itself, the methodology for this algorithim is going to be slightly different. We'll do the following:\n",
    "\n",
    "* Cleans data (Removes stopwords, stems, etc.)\n",
    "* Tokenizes words, might TF-IDF it\n",
    "* Perform NMF on tokenized vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model Performance\n",
    "\n",
    "Measuring the performance of a model is typically quite trival. However, since the model used on actual data will have no labels reflecting the true topic or cluster, we're trying to measure something that doesn't really exist.\n",
    "\n",
    "Having absolutely no labelled data adds a complexity in choosing the model, and really only leaves us with The Silhouette Coefficient or the Calinski-Harabaz index. Both are metrics where evaluation is performed on the model itself without ground label truths (labelled data). In this analysis, we'll be using the Silhouette Coefficient.\n",
    "\n",
    "The Silhouette Coefficient can is a measure of how well defined the clusters are, from this we can infer as to whether there are seperate clusters, or whether the clusters are overlapping. It's calculated by:\n",
    "\n",
    "\\begin{align}\n",
    "s = \\frac{b-a}{\\max (b)}\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "* a: The mean distance between a sample and all other points in the same class.\n",
    "* b: The mean distance between a sample and all other points in the next nearest cluster.\n",
    "\n",
    "### Interpreting the Performance\n",
    "\n",
    "The Silhouette Coefficient is range bound to -1 to +1, where a high value indicates the object is well matched to its own cluster. So we are looking to maximise the Sillhouette Coefficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0050303421121108355"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.silhouette_score(X, k_means_labels, metric='euclidean')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
